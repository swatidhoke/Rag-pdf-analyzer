{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d334d1",
   "metadata": {},
   "source": [
    "### Tools for building the RAG App\n",
    "\n",
    "Now that we are familiar with the overall architecture, we can now go ahead and structure the tools that we'll use for the upcoming demonstration:\n",
    "\n",
    "- OpenAI LLM (model - GPT 4o-mini): This will be our primary model for generating the responses\n",
    "- LangChain: Langchain is a powerful framework for orchestrating different layers in the RAG app. We shall use this to build the retriever end-to-end and also connect with other tools for tasks such as\n",
    "    - Chunking - RecursiveCharacterTextSplitter\n",
    "    - Embedding Model - Openapi\n",
    "    - Vector Search Model - FAISS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969225f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swatisalunkke/SSRAG/rag/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/swatisalunkke/SSRAG/rag/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders  import PyPDFLoader, PyMuPDFLoader , DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Importing ChatOpenAI from LangChain to interact with OpenAI's language models,\n",
    "# such as GPT, for generating responses.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Importing ChatPromptTemplate to create structured prompts for the chatbot,\n",
    "# ensuring consistent interactions with the AI model.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Importing OpenAIEmbeddings to convert text data into numerical vector\n",
    "# representations for similarity search and retrieval.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Importing create_stuff_documents_chain to combine and process retrieved\n",
    "# documents for meaningful AI-generated responses.\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Importing create_retrieval_chain to build a chain that retrieves relevant\n",
    "# documents from a vector store and generates AI responses.\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12479c63",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 1. Load PDF\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d601d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 PDF files in the directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_pdf_documents(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_directory = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_directory.glob('**/*.pdf'))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in the directory.\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file}\")\n",
    "        loader = PyMuPDFLoader(str(pdf_file))\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"\\nTotal document loaded so far: {len(all_documents)}\\n\")\n",
    "    return all_documents\n",
    "\n",
    "#Process the PDF documents in the specified directory\n",
    "processed_documents = process_pdf_documents(\"../data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3851dea",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 2. Split documents into chunks\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758c5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_into_chunks(documents, chunk_size=1000, chunk_overlap=500):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    print(f\"Split..... {len(documents)} documents into.... {len(split_documents)} chunks\")\n",
    "\n",
    "    if split_documents:\n",
    "        print(f\"Sample chunk (first 500 characters):\\n{split_documents[0].page_content[:500]}\")\n",
    "        print(f\"\\nLength of the sample chunk: {len(split_documents[0].metadata)} characters\")\n",
    "    return split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79a71d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split..... 0 documents into.... 0 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents_into_chunks(processed_documents)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ecb0b2",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 3. Create embeddings\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea57cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00676333112642169, -0.03919631987810135, 0.034175805747509, 0.02876211516559124, -0.02478501945734024]\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # loads .env into environment\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "print(embeddings.embed_query(\"hello world\")[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eca004",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 4. Create or load FAISS index\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda51362",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaiss_index_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SSRAG/rag/lib/python3.14/site-packages/langchain_community/vectorstores/faiss.py:1190\u001b[39m, in \u001b[36mFAISS.load_local\u001b[39m\u001b[34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[39m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_dangerous_deserialization:\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe de-serialization relies loading a pickle file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPickle files can be modified to deliver a malicious payload that \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresults in execution of arbitrary code on your machine.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1194\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou will need to set `allow_dangerous_deserialization` to `True` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33menable deserialization. If you do this, make sure that you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1196\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrust the source of the data. For example, if you are loading a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile that you created, and know that no one else has modified the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile, then this is safe to do. Do not set this to `True` if you are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloading a file from an untrusted source (e.g., some random site on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe internet.).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1201\u001b[39m     )\n\u001b[32m   1202\u001b[39m path = Path(folder_path)\n",
      "\u001b[31mValueError\u001b[39m: The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     vectorstore = FAISS.load_local(faiss_index_path, embeddings)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     vectorstore.save_local(faiss_index_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SSRAG/rag/lib/python3.14/site-packages/langchain_core/vectorstores/base.py:814\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    812\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SSRAG/rag/lib/python3.14/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SSRAG/rag/lib/python3.14/site-packages/langchain_community/vectorstores/faiss.py:1001\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    998\u001b[39m     index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     index = faiss.IndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[32m   1002\u001b[39m docstore = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdocstore\u001b[39m\u001b[33m\"\u001b[39m, InMemoryDocstore())\n\u001b[32m   1003\u001b[39m index_to_docstore_id = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mindex_to_docstore_id\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(f\"Number of document chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk text: {chunks[0].page_content[:200]}\")\n",
    "faiss_index_path = \"faiss_index\"\n",
    "\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(faiss_index_path, embeddings)\n",
    "except:\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426f9ef",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 5. Create retriever\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0789c",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 6. Create RAG chain\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ef942",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8337a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Summarize the following document content into a concise, readable summary:\n",
    "Document Content:\n",
    "{context}\n",
    "Summary:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 1: create a map-reduce style chain for combining multiple docs\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt=prompt, output_parser=StrOutputParser())\n",
    "\n",
    "# Step 2: create the retrieval chain with your retriever\n",
    "qa_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=doc_chain)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e76a2b",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 7. Ask a question\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a61a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize the document\"\n",
    "response = qa_chain.invoke({\"input\": query})\n",
    "print(\"Summary:\", response['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
