{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d334d1",
   "metadata": {},
   "source": [
    "### Tools for building the RAG App\n",
    "\n",
    "Now that we are familiar with the overall architecture, we can now go ahead and structure the tools that we'll use for the upcoming demonstration:\n",
    "\n",
    "- OpenAI LLM (model - GPT 4o-mini): This will be our primary model for generating the responses\n",
    "- LangChain: Langchain is a powerful framework for orchestrating different layers in the RAG app. We shall use this to build the retriever end-to-end and also connect with other tools for tasks such as\n",
    "    - Chunking - RecursiveCharacterTextSplitter\n",
    "    - Embedding Model - Openapi\n",
    "    - Vector Search Model - FAISS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969225f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swatisalunkke/SSRAG/rag/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/swatisalunkke/SSRAG/rag/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders  import PyPDFLoader , DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Importing ChatOpenAI from LangChain to interact with OpenAI's language models,\n",
    "# such as GPT, for generating responses.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Importing ChatPromptTemplate to create structured prompts for the chatbot,\n",
    "# ensuring consistent interactions with the AI model.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Importing OpenAIEmbeddings to convert text data into numerical vector\n",
    "# representations for similarity search and retrieval.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Importing create_stuff_documents_chain to combine and process retrieved\n",
    "# documents for meaningful AI-generated responses.\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Importing create_retrieval_chain to build a chain that retrieves relevant\n",
    "# documents from a vector store and generates AI responses.\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12479c63",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 1. Load PDF\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d601d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files in the directory.\n",
      "Processing file: ../data/AI_ML_Swati_Salunkhe-July2025.pdf\n",
      "\n",
      "Total document loaded so far: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_pdf_documents(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_directory = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_directory.glob('**/*.pdf'))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in the directory.\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing file: {pdf_file}\")\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"\\nTotal document loaded so far: {len(all_documents)}\\n\")\n",
    "    return all_documents\n",
    "\n",
    "#Process the PDF documents in the specified directory\n",
    "processed_documents = process_pdf_documents(\"../data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3851dea",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 2. Split documents into chunks\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758c5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_into_chunks(documents, chunk_size=1000, chunk_overlap=500):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    print(f\"Split..... {len(documents)} documents into.... {len(split_documents)} chunks\")\n",
    "\n",
    "    if split_documents:\n",
    "        print(f\"Sample chunk (first 500 characters):\\n{split_documents[0].page_content[:500]}\")\n",
    "        print(f\"\\nLength of the sample chunk: {len(split_documents[0].metadata)} characters\")\n",
    "    return split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79a71d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split..... 4 documents into.... 23 chunks\n",
      "Sample chunk (first 500 characters):\n",
      "SWATI  SALUNKHE  www.linkedin.com/in/swatidhoke  https://github.com/swatidhoke                    619-581-8608  San  Jose,  CA  -  95128                                                                                                                               swatisalunkhe185@gmail.com  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     \n",
      " \n",
      " \n",
      "  SUMMARY   AI-Focused  Software  Engineer  with  12+  years  of  experience  across  full-stack  development  and  Agile  environments.  \n",
      "Specialized\n",
      " \n",
      "in\n",
      " \n",
      "validating\n",
      " \n",
      "and\n",
      " \n",
      "optimizing\n",
      " \n",
      "\n",
      "\n",
      "Length of the sample chunk: 8 characters\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents_into_chunks(processed_documents)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ecb0b2",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 3. Create embeddings\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea57cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00676333112642169, -0.03919631987810135, 0.034175805747509, 0.02876211516559124, -0.02478501945734024]\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # loads .env into environment\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "print(embeddings.embed_query(\"hello world\")[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eca004",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 4. Create or load FAISS index\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda51362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 23\n",
      "Sample chunk text: SWATI  SALUNKHE  www.linkedin.com/in/swatidhoke  https://github.com/swatidhoke                    619-581-8608  San  Jose,  CA  -  95128                                                                \n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of document chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk text: {chunks[0].page_content[:200]}\")\n",
    "faiss_index_path = \"faiss_index\"\n",
    "\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(faiss_index_path, embeddings)\n",
    "except:\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426f9ef",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 5. Create retriever\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2f105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0789c",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 6. Create RAG chain\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104ef942",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8337a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Summarize the following document content into a concise, readable summary:\n",
    "Document Content:\n",
    "{context}\n",
    "Summary:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 1: create a map-reduce style chain for combining multiple docs\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt=prompt, output_parser=StrOutputParser())\n",
    "\n",
    "# Step 2: create the retrieval chain with your retriever\n",
    "qa_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=doc_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e76a2b",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# 7. Ask a question\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a61a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The document outlines the professional experience of a Software QA Engineer with roles at Symantec Corporation, Refresh Inc., and Hewlett-Packard. \n",
      "\n",
      "At Symantec (Apr 2014 - Apr 2015), the engineer worked on the Symantec App Center, focusing on mobile application and content management. Responsibilities included monitoring code coverage with SonarQube, writing JAVA integration tests using Seismic and RestAssured, conducting platform upgrade testing, and performing accessibility testing with tools like AXE and JAWS. The engineer also wrote SQL queries for database interaction.\n",
      "\n",
      "At Refresh Inc. (July 2013 - Apr 2014), the engineer was the primary developer for automated test design and feature implementation, creating a test automation framework using Xcode, JavaScript, XHTML, and XML. They tested various external APIs and maintained unit test cases in TestNG, while utilizing continuous integration tools like Jenkins, Maven, and GIT.\n",
      "\n",
      "The document also mentions a previous role at Hewlett-Packard (Nov 2011 - Jun 2013), but details of that position are not provided.\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize the document\"\n",
    "response = qa_chain.invoke({\"input\": query})\n",
    "print(\"Summary:\", response['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
